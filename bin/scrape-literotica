#!/usr/bin/env python3
import argparse
import bs4
import json
import os
from pprint import pprint
import re
from nullroute.core import Core
from nullroute.file import set_file_attr
from nullroute.string import escape_html, filter_filename
from nullroute.scrape import Scraper

class LiteroticaScraper(Scraper):
    def subclass_init(self):
        self.ua.headers["User-Agent"] = "Mozilla/5.0 (X11; Linux x86_64; rv:90.0) Gecko/20100101 Firefox/90.0"

    def get_story_by_stub(self, story_id, page=1):
        resp = self.get(f"https://literotica.com/api/3/stories/{story_id}",
                        params={"params": json.dumps({"contentPage": page})})
        return resp.json()

    def scrape_story_page(self, page_url):
        page_url = page_url.replace("http://", "https://")
        page_url = page_url.replace("/stories/showstory.php?url=", "/s/")
        resp = self.get(page_url)
        return self.parse_story_page(resp.content)

    def parse_story_page(self, buf):
        data = {}
        page = bs4.BeautifulSoup(buf, "lxml")

        if tag := page.select_one("div.page__main > div.panel.article > div.aa_ht > div"):
            data["body"] = str(tag)
        else:
            raise RuntimeError("could not find body text in page (no div.article div.aa_ht)")

        return data

    def _url_to_filename(self, url, author=None):
        url = url.split("?")[0]
        url = url.split("/")[-1]
        url = filter_filename(url)
        if author and opts.author:
            url += "_(%s)" % filter_filename(author)
        return "%s.html" % url

    def save_page_data(self, data):
        Core.debug("got \"%(title)s\" (page %(page_cur)d out of %(page_max)d)" % data)

        filename = self._url_to_filename(data["url"], data["author"])
        Core.debug("saving to %r" % filename)

        if data["page_cur"] == 1:
            title = "%(title)s, by %(author)s" % data
            with open(filename, "w") as fh:
                Core.trace("writing HTML header")
                fh.write('<!DOCTYPE html>\n')
                fh.write('<head>\n')
                fh.write('\t<meta charset="utf-8">\n')
                fh.write('\t<title>%s</title>\n' % escape_html(title))
                fh.write('\t<link rel="stylesheet" href="../literotica.css">\n')
                fh.write('</head>\n')
                fh.write('<div class="header">\n')
                fh.write('\t<h1>%s</h1>\n' % escape_html(data["title"]))
                fh.write('\t<p class="meta">\n')
                fh.write('\t\tby <a href="%s">%s</a>\n' % (
                            escape_html(data["author_url"]),
                            escape_html(data["author"]),
                        ))
                fh.write('\t\ton <a href="%s">%s</a>\n' % (
                            escape_html(data["url"]),
                            "Literotica",
                        ))
                fh.write('\t</p>\n')
                fh.write('</div>\n')
                fh.write('<div class="content">\n')
            set_file_attr(filename, "xdg.origin.url", data["url"])
            set_file_attr(filename, "xdg.referrer.url", data["author_url"])
            set_file_attr(filename, "dublincore.title", data["title"])
            set_file_attr(filename, "dublincore.creator", data["author"])

        with open(filename, "a") as fh:
            Core.trace("appending story body")
            fh.write('\t<!-- page %(page_cur)s out of %(page_max)s -->\n' % data)
            fh.write('\t<div class="page" id="page%(page_cur)s">\n' % data)
            fh.write('\t\t%(body)s\n' % data)
            fh.write('\t</div>\n')

        if data["page_cur"] == data["page_max"]:
            with open(filename, "a") as fh:
                Core.trace("appending HTML footer")
                fh.write('</div>\n')
                if data["also_series"]:
                    fh.write('<div class="also">\n')
                    fh.write('\t<hr>\n')
                    fh.write('\t<h3>Also in this series</h3>\n')
                    fh.write('\t<ul>\n')
                    for also_url, also_title in data["also_series"]:
                        also_file = self._url_to_filename(also_url, data["author"])
                        fh.write('\t\t<li><a href="%s">%s</a>\n' % (
                                    escape_html(also_file),
                                    escape_html(also_title),
                                ))
                        fh.write('\t\t\t<small>(<a href="%s">%s</a>)</small>\n' % (
                                    escape_html(also_url),
                                    "online",
                                ))
                    fh.write('\t</ul>\n')
                    fh.write('</div>\n')
                fh.write('<!-- end of story -->\n')
            data["filename"] = filename
            Core.info("saved \"%(title)s\" (%(page_max)s pages) to \"%(filename)s\"" % data)

    '''
    def save_story(self, url, series=False):
        data = {"page_next": url.split("?")[0]}
        while data["page_next"]:
            data = self.scrape_story_page(data["page_next"])
            self.save_page_data(data)

        if series and data["also_series"]:
            Core.info("found %d more stories in series" % len(data["also_series"]))
            for also_url, also_title in data["also_series"]:
                self.save_story(also_url)
    '''

    def save_story(self, story_url, series=False):
        if m := re.match(r"^https?://www\.literotica\.com/s/([^?]+)", story_url):
            story_stub = m.group(1)
            story_url = f"https://www.literotica.com/s/{story_stub}"
        else:
            Core.err("unrecognized URL: %r", story_url)
            return False

        Core.info("getting story %r", story_stub)
        r = self.get_story_by_stub(story_stub)
        pages = r["meta"]["pages_count"]
        info = r["submission"]

        data = {
            "title": info["title"],
            "url": story_url,
            "author": info["authorname"],
            "author_url": f"https://www.literotica.com/stories/memberpage.php?uid={info['author']['userid']}&page=submissions",
            "page_max": pages,
            "also_series": [(f"https://www.literotica.com/s/{a['url']}", a["title"])
                            for a in info.get("series", {}).get("items", [])],
        }

        # The API provides not-quite-HTML which we can't use, such as:
        # "<i>Foo\n\nBar\n\nBaz</i>"
        """
        text = r["pageText"] + "\n"
        for page in range(2, pages+1):
            Core.info("... page %d of %d", page, pages)
            r = self.get_story_by_stub(story_stub, page)
            text += "\r\n" + r["pageText"] + "\n"
        """

        for page in range(1, pages+1):
            Core.info("... page %d of %d", page, pages)
            page_data = self.scrape_story_page(f"{story_url}?page={page}")
            self.save_page_data(data | page_data | {"page_cur": page})

        if series and (also := data["also_series"]):
            Core.info("found %d more stories in series", len(also)-1)
            for also_url, also_title in also:
                if also_url != story_url:
                    self.save_story(also_url)

parser = argparse.ArgumentParser()
#parser.add_argument("-s", "--series",
#                    dest="series", action="store_true", default=False,
#                    help="download all related stories in the series")
parser.add_argument("-S", "--no-series",
                    dest="series", action="store_false", default=True,
                    help="only download given story, not the entire series")
parser.add_argument("--name-author",
                    dest="author", action="store_true", default=False,
                    help="include author in filename")
parser.add_argument("url", nargs="*")
opts = parser.parse_args()

foo = LiteroticaScraper()
for url in opts.url:
    if os.path.exists(url):
        with open(url, "r") as fh:
            buf = fh.read()
        data = foo.parse_story_page(buf)
        pprint(data)
    else:
        foo.save_story(url, series=opts.series)
