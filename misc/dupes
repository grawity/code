#!/usr/bin/env python3
# encoding: utf-8
import os
import sys
import stat
import binascii
import fnmatch
import hashlib
import io
import math
from argparse import ArgumentParser
from collections import defaultdict
from nullroute.core import *
from nullroute.misc import escape_shell, fmt_size
from nullroute.ui import print_status, stderr_tty

opts = None

header_size = 512
file_sizes = {}     # path → size (stage 1)
file_headers = {}   # path → header (stage 2)
file_hashes = {}    # path → hash (stage 3)
total_files = 0
unique_files = 0
total_wasted = 0
files_removed = 0
size_removed = 0
dirs_skipped = 0

_last_status = 0

if Core._log_level >= Core.LOG_TRACE:
    def print_status(*args):
        Core.trace(" ".join(args))

def status(*args):
    if stderr_tty() and not opts.verbose:
        import time
        global _last_status
        print_status(*args)
        now = time.time()
        if (not args) or (now - _last_status > 0.1):
            sys.stderr.flush()
            _last_status = now

def path_is_ignored(path):
    for filter in opts.ignore:
        if fnmatch.fnmatch(path, filter):
            return True
    return False

def path_is_removable(path):
    for filter in opts.keep:
        if fnmatch.fnmatch(path, filter):
            return False
    for filter in opts.remove:
        if fnmatch.fnmatch(path, filter):
            return True
    return False

def path_is_perishable(path):
    for filter in opts.only:
        if fnmatch.fnmatch(path, filter):
            return True
    return False

def enum_files(root_dir):
    global dirs_skipped
    ignores = {".git", ".hg", ".sync"}
    Core.debug("enumerating %r" % root_dir)
    if os.path.isdir(root_dir):
        for subdir, dirs, files in os.walk(root_dir):
            if ".nodupes" in files and subdir != "." and not opts.all:
                Core.debug("skipping %r (found .nodupes marker)", subdir)
                dirs.clear()
                dirs_skipped += 1
                continue
            for item in dirs[:]:
                if item in ignores:
                    Core.debug("skipping %r/%r (ignored directory)", subdir, item)
                    dirs.remove(item)
            for name in files:
                path = os.path.join(subdir, name)
                if path_is_ignored(path):
                    continue
                yield path
    else:
        if not path_is_ignored(root_dir):
            yield root_dir
        else:
            Core.debug("skipping %r (root directory matches ignore)")

def get_header(path):
    if path not in file_headers:
        if opts.verbose:
            print("reading", path, file=sys.stderr)
        with open(path, "rb") as fh:
            file_headers[path] = hashlib.sha1(fh.read(header_size)).digest()
    return file_headers[path]

def hash_file(path, status_func=None):
    if path not in file_hashes:
        if opts.verbose:
            print("hashing", path, file=sys.stderr)
        h = hashlib.sha1()
        buf_size = 4 * 1024 * 1024
        n_bytes = os.stat(path).st_size
        with open(path, "rb") as fh:
            if status_func and n_bytes > 0:
                n_hashed = 0
                status_func(0.0)
            buf = True
            while buf:
                buf = fh.read(buf_size)
                h.update(buf)
                if status_func and n_bytes > 0:
                    n_hashed += len(buf)
                    status_func(n_hashed / n_bytes * 100)
        file_hashes[path] = h.digest()
    return file_hashes[path]

def fmt_hash(hash):
    return binascii.b2a_hex(hash).decode("utf-8")

def unfmt_size(sz, si=False):
    if not sz:
        return -1
    prefixes = "kMGTPE"
    mult = 1000 if si else 1024
    pos = prefixes.index(sz[-1])
    val = float(sz[:-1])
    exp = pos + 1
    return val * (mult ** exp)

def find_duplicates(root_dirs, only_globs=None):
    known_sizes = defaultdict(list)
    known_headers = defaultdict(list)
    known_hashes = defaultdict(list)

    n_size = 0
    n_head = 0
    n_hash = 0

    # find files identical in size
    for root_dir in root_dirs:
        for path in enum_files(root_dir):
            n_size += 1
            status("stat (%d)" % n_size, path)
            st = os.lstat(path)
            if not stat.S_ISREG(st.st_mode):
                continue
            #if st.st_size < opts.larger_than:
            #    continue
            file_sizes[path] = st.st_size
            known_sizes[st.st_size].append(path)

    status("stat/prune")

    # skip duplicates if none of the paths match --only=
    if only_globs:
        for size, paths in known_sizes.items():
            if len(paths) > 1:
                if not any([fnmatch.fnmatch(p, g)
                            for p in paths
                            for g in only_globs]):
                    known_sizes[size] = []

    status("stat/head")

    # find files identical in size and first `header_size` bytes
    head_todo = []

    for size, paths in known_sizes.items():
        if len(paths) > 1:
            head_todo += paths

    for path in sorted(head_todo):
        n_head += 1
        status("head (%d/%d)" % (n_head, len(head_todo)), path)
        try:
            header = get_header(path)
        except FileNotFoundError:
            status()
            Core.notice("file %r disappeared mid-scan", path)
            continue
        known_headers[size, header].append(path)

    status("head/hash")

    # find files identical in size and hash

    total_size = 0
    hashed_size = 0

    for (size, header), paths in known_headers.items():
        total_size += size * len(paths)

    for (size, header), paths in known_headers.items():
        if len(paths) < 2:
            n_hash += 1
            hashed_size += size
            continue

        if size <= header_size:
            # optimization: don't compare by hash if
            # the entire contents are already known
            n_hash += len(paths)
            hashed_size += size * len(paths)
            status()
            yield paths
            continue

        for path in paths:
            n_hash += 1
            status("hash %d%% (%d/%d)" % (hashed_size / total_size * 100, n_hash, n_head), path)
            try:
                filehash = hash_file(path,
                    lambda p: status("hash %d%% (%d/%d, %d%%)" % ((hashed_size + size * (p / 100)) / total_size * 100, n_hash, n_head, p), path))
            except FileNotFoundError:
                status()
                Core.notice("file %r disappeared mid-scan", path)
                continue
            hashed_size += size
            known_hashes[size, filehash].append(path)

    status()

    res = []
    for (size, filehash), paths in known_hashes.items():
        if len(paths) < 2:
            continue
        res.append(paths)
    res.sort()
    yield from res

if __name__ == "__main__":
    sys.stdout = io.TextIOWrapper(sys.stdout.detach(),
                                  encoding="utf-8",
                                  errors="surrogateescape")

    ap = ArgumentParser()
    ap.add_argument("--all",
                    dest="all", action="store_true", default=False,
                    help="disregard .nodupes tags in directories")
    ap.add_argument("-v", "--verbose",
                    dest="verbose", action="store_true", default=False,
                    help="show files as they are processed")
    ap.add_argument("-l", "--list",
                    dest="list", action="store_true", default=False,
                    help="output files as a sortable list")
    ap.add_argument("-Q", "--quote",
                    dest="quote", action="store_true", default=False,
                    help="quote paths for shell")
    ap.add_argument("--ignore", metavar="GLOB",
                    dest="ignore", action="append", default=[],
                    help="ignore matching paths entirely")
    ap.add_argument("--keep", metavar="GLOB",
                    dest="keep", action="append", default=[],
                    help="always keep matching paths")
    ap.add_argument("--remove", metavar="GLOB",
                    dest="remove", action="append", default=[],
                    help="automatically remove matching paths")
    ap.add_argument("--only", metavar="GLOB",
                    dest="only", action="append", default=[],
                    help="only list files with duplicates matching given paths")
    ap.add_argument("--larger-than", metavar="SIZE",
                    dest="larger_than",
                    help="TODO")
    ap.add_argument("path", nargs="*")

    opts = ap.parse_args()

    opts.larger_than = unfmt_size(opts.larger_than)

    root_dir = opts.path[:] or ["."]

    try:
        for paths in find_duplicates(root_dir, opts.only):
            if opts.only and not any(path_is_perishable(p) for p in paths):
                continue
            paths.sort()
            size = file_sizes[paths[0]]
            hash = hash_file(paths[0])
            num = len(paths)
            wasted = size * (num - 1)
            if opts.list and opts.remove:
                for path in paths:
                    if num > 1 and path_is_removable(path):
                        print("rm -vf %s" % escape_shell(path))
                        num -= 1
            elif opts.list:
                for path in paths:
                    print(wasted, fmt_hash(hash), path)
            else:
                print("\033[38;5;11mDuplicates (%s wasted):\033[m" % fmt_size(wasted))
                for path in paths:
                    qpath = escape_shell(path) if opts.quote else path
                    if num > 1 and path_is_removable(path):
                        print("   \033[1m\033[38;5;9m×\033[m", qpath)
                        os.unlink(path)
                        files_removed += 1
                        size_removed += size
                        num -= 1
                        wasted -= size
                    else:
                        print("    ", qpath)
            total_files += num
            unique_files += 1
            total_wasted += wasted
    except KeyboardInterrupt:
        status()
        Core.notice("scan interrupted")

    sys.stdout.flush()

    if opts.list and opts.remove:
        pass
    elif opts.verbose or opts.list:
        wasted_files = total_files - unique_files
        print("; %d files compared by header" % len(file_headers))
        print("; %d files compared by hash" % len(file_hashes))
        print("; %s wasted by %d duplicates" % (fmt_size(total_wasted), wasted_files))
        if dirs_skipped:
            print("; %d directories skipped" % dirs_skipped)
    else:
        if dirs_skipped:
            Core.notice("%d directories skipped" % dirs_skipped)
        if files_removed:
            Core.info("%s files removed, %s saved" % (
                files_removed,
                fmt_size(size_removed),
            ))
        if total_files:
            Core.info("%s files %s (%s unique), %s wasted" % (
                total_files,
                "remain" if files_removed else "found",
                unique_files,
                fmt_size(total_wasted),
            ))
