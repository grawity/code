#!/usr/bin/env python
from argparse import ArgumentParser
import bs4
from nullroute.core import *
from nullroute.misc import escape_html, filter_filename, set_file_attr
from nullroute.scrape import Scraper, urljoin
from pprint import pprint
import re
import sys

def filter_body(story):
    # from ffnet-convert.pl

    # clean up whitespace
    story = re.sub(r"\s\s+", r" ", story)

    # change tags to lowercase
    story = re.sub(r"</?[A-Z]+>", lambda x: x.lower(), story)

    # split paragraphs to their own lines
    story = re.sub(r"(</p>)", r"\1\n", story)

    # fixup <br> tags, break line after tag
    """
#	$story =~ s|\s*<br\s*/?>\s*|<br>\n|g;
    """

    # split giant <br> mess into paragraphs
    """
#	if ($story !~ m|<p>|) {
#		$story =~ s|<br>\n<br>\n|</p>\n<p>|g;
#		$story =~ s|^|<p>|;
#		$story =~ s|$|</p>|;
#	}
#   """

    # split <hr>s into their own lines, destyle them
    """
    $story =~ s|<hr[^>]+>|<hr>|g;
    $story =~ s|(<hr>)(<p>.+)|$1\n$2|g;
    $story =~ s|(<hr>)(?!<p>)(.+)|$1\n<p>$2|g;
    """
    story = re.sub(r"<hr[^>]+>", "<hr>", story)
    story = re.sub(r"(<hr>)(<p>.+)", r"\1\n\2", story)
    story = re.sub(r"(<hr>)(?!<p>)(.+)", r"\1\n<p>\2", story)

    # unwrap multi-line paragraphs
    """
    sub unwrap { my $str = shift; $str =~ s/\s*\n\s*/ /g; return $str; }
#	$story =~ s|^(<p[^>]*>.+?</p>)$|unwrap($1)|gmse;
    """
    def unwrap(m):
        t = m.group(1)
        t = re.sub(r"\s*\n\s*", "", t)
        return t
    story = re.sub(r"^(<p[^>]*>.+?</p>)$", unwrap, story, re.M | re.S)

    # split paragraphs to their own lines, 2nd pass
    """
    $story =~ s|(\S)\s*(<p>)|$1\n$2|g;
    """

    # close paragraph tags
    """
    $story =~ s|^(<p>[^>]+)(?!</p>)$|$1</p>|mg;
    """

    # fix <p><em> blocks
    """
    $story =~ s|^((?:<p><em>[^<]+</em></p>\n){6,})|unstrong($1)|msge;
    """

    # fix separators
    """
    $story =~ s|^<p style=['"]text-align:\s*center;?['"]>([^A-Z]+)</p>$|<p class="ast">$1</p>|mg;
    """

    # typography – ellipses
    """
    $story =~ s/\.\.\./…/g;
    """

    # typography – apostrophes
    """
    $story =~ s/´/’/g;
    $story =~ s/(\w+)'(d|ll|m|re|s|ve)\b/$1’$2/g;
    $story =~ s/(\w+n)'(t)\b/$1’$2/g;
    $story =~ s/(o)'(clock)/$1’$2/g;
    """

    # typography – double quotes
    """
    $story =~ s/([ >])"([^ <.,!?])/$1“$2/g;
    $story =~ s/([^ >])"([ <.,!?])/$1”$2/g;
    """

    # (surrounding tags)
    """
    $story =~ s/([ >])"(<[^\/][^>]+>)/$1“$2/g;
    $story =~ s/(<\/[^>]+>)"([ <])/$1”$2/g;
    """

    # typography – dashes
    """
    $story =~ s/(\s)--(\s)/$1–$1/g; # en dash
    $story =~ s/(\S)--(\S)/$1—$2/g; # em dash
    $story =~ s/(\w)-+(["”'’])/$1–$2/g; # em dash (to match above)
    """

    return story

class UnknownSiteError(Exception):
    pass

class FictionPressScraper(Scraper):
    def _fix_url(self, url):
        if url.startswith("//"):
            return f"https:{url}"
        else:
            return url

    def parse_story_page(self, page):
        data = {}

        tag = page.find("title")
        data["site_name"] = tag.get_text().split("|")[1].strip()

        tag = page.find("link", {"rel": "canonical"})
        data["url"] = self._fix_url(tag["href"])

        tag = page.find("input", {"name": "uid"})
        data["author_id"] = tag["value"]

        tag = page.find("input", {"name": "sid"})
        data["story_id"] = tag["value"]

        tag = page.find("input", {"name": "ch"})
        data["page_cur"] = int(tag["value"])

        tag = page.select_one("form[name=j] script")
        m = re.search(r"var chs = (\d+);", str(tag))
        data["page_max"] = int(m.group(1))

        tag = page.select_one("div#content div[align=center] b")
        data["title"] = tag.get_text(strip=True)

        tag = page.select_one("div#content div[align=center] a")
        data["author"] = tag.get_text(strip=True)
        data["author_url"] = urljoin(data["url"], tag["href"])

        tag = page.select_one("div#storycontent")
        data["body"] = str(tag)

        return data

    def scrape_story_page(self, page_url):
        # XXX: should do the opposite here instead
        page_url = page_url.replace("//www.", "//m.")
        page = self.get_page(page_url)
        data = self.parse_story_page(page)
        return data

    def _url_to_filename(self, url, author=None):
        url = url.split("?")[0]
        url = url.split("/")[-1]
        url = filter_filename(url)
        if author:
            url += "_(%s)" % filter_filename(author)
        return "%s.html" % url

    def save_chapter_data(self, data):
        Core.debug("got \"%(title)s\" (chapter %(page_cur)d out of %(page_max)d)" % data)

        filename = self._url_to_filename(data["url"])
        Core.debug("saving to %r" % filename)

        data["body"] = filter_body(data["body"])

        if data["page_cur"] == 1:
            title = "%(title)s, by %(author)s" % data
            with open(filename, "w") as fh:
                Core.trace("writing HTML header")
                fh.write('<!DOCTYPE html>\n')
                fh.write('<head>\n')
                fh.write('\t<meta charset="utf-8">\n')
                fh.write('\t<title>%s</title>\n' % escape_html(title))
                #fh.write('\t<link rel="stylesheet" href="../literotica.css">\n')
                fh.write('</head>\n')
                fh.write('<div class="header">\n')
                fh.write('\t<h1>%s</h1>\n' % escape_html(data["title"]))
                fh.write('\t<p class="meta">\n')
                fh.write('\t\tby <a href="%s">%s</a>\n' % (
                            escape_html(data["author_url"]),
                            escape_html(data["author"]),
                        ))
                fh.write('\t\ton <a href="%s">%s</a>\n' % (
                            escape_html(data["url"]),
                            escape_html(data["site_name"]),
                        ))
                fh.write('\t</p>\n')
                fh.write('</div>\n')
                fh.write('<div class="content">\n')
            set_file_attr(filename, "xdg.origin.url", data["url"])
            set_file_attr(filename, "xdg.referrer.url", data["author_url"])
            set_file_attr(filename, "dublincore.title", data["title"])
            set_file_attr(filename, "dublincore.creator", data["author"])

        with open(filename, "a") as fh:
            Core.err("XXX: WHERE ARE THE CHAPTER TITLES")
            Core.trace("appending story body")
            fh.write('\t<!-- page %(page_cur)s out of %(page_max)s -->\n' % data)
            fh.write('\t<div class="page" id="page%(page_cur)s">\n' % data)
            fh.write('\t\t%(body)s\n' % data)
            fh.write('\t</div>\n')

        if data["page_cur"] == data["page_max"]:
            with open(filename, "a") as fh:
                Core.trace("appending HTML footer")
                fh.write('</div>\n')
                fh.write('<!-- end of story -->\n')
            data["filename"] = filename
            Core.info("saved \"%(title)s\" (%(page_max)s pages) to \"%(filename)s\"" % data)

    def save_raw_page(self, data, buf):
        slug = os.path.basename(data["url"])
        os.makedirs(slug, exist_ok=True)

        filename = "%s/%s.html" % (slug, data["page_cur"])
        with open(filename, "wb") as fh:
            fh.write(buf)
            data["filename"] = filename
            Core.info("saved page %(page_cur)s of %(page_max)s to \"%(filename)s\"" % data)

    def save_story(self, url, save_raw=False):
        m = re.match(r"^https://(m|www)\.[^:/]+/s/\d+/", url)
        base_url = m.group(0)
        page_url = "%s%d/" % (base_url, 1)
        while True:
            # XXX: should do the opposite here instead
            page_url = page_url.replace("//www.", "//m.")
            Core.info("downloading %r" % page_url)
            resp = self.get(page_url)
            page = bs4.BeautifulSoup(resp.content, "lxml")
            data = self.parse_story_page(page)

            if save_raw:
                self.save_raw_page(data, resp.content)
            else:
                self.save_chapter_data(data)

            if data["page_cur"] < data["page_max"]:
                page_url = "%s%d/" % (base_url, data["page_cur"] + 1)
            else:
                break

ap = ArgumentParser()
ap.add_argument("url", nargs="*")
opts = ap.parse_args()
foo = FictionPressScraper()
for url in opts.url:
    foo.save_story(url)
