#!/usr/bin/env python
# scrape-reddit-comment - scraper for Reddit comments

import bs4
import datetime
import json
import os
import re
import requests
import sys
import time

rx_time = re.compile(r'(\d+)-(\d+)-(\d+)T(\d+):(\d+):(\d+)(?:\.(\d+))?([+-]\d+:\d+)')

rx_msgid = re.compile(r'.*/comments/(\w+)/.*/(\w+)/?$')

DELTA_ZERO = datetime.timedelta(0)

class FixedOffset(datetime.tzinfo):
    def __init__(self, stroffset):
        self._name = stroffset
        self._offset = datetime.timedelta(hours=int(stroffset[:3]),
                                          minutes=int(stroffset[4:6]))

    def utcoffset(self, dt):
        return self._offset

    def tzname(self, dt):
        return self._name

    def dst(self, dt):
        return DELTA_ZERO

def scrape(url):
    req = requests.get(url + "?limit=1", headers={"User-Agent": "Mozilla/5.0"})
    body = bs4.BeautifulSoup(req.text)

    title = body.find("title").get_text()

    commentarea = body.find("div", {"class": "commentarea"})
    infobar = commentarea.find("div", {"class": "infobar"})

    if infobar is None:
        thing = body.select("div.thing.link")[0]
    else:
        thing = commentarea

    entry = thing.find("div", {"class": "entry"})

    # .entry > .tagline > {.author, time}

    tagline = entry.find("p", {"class": "tagline"})

    author = tagline.find("a", {"class": "author"})
    if author:
        author = author.get_text()
    else:
        author = '"(deleted user)"'

    strtime = tagline.find("time")["datetime"]

    mx = rx_time.match(strtime)
    if mx:
        ntime = datetime.datetime(
                    year=int(mx.group(1)),
                    month=int(mx.group(2)),
                    day=int(mx.group(3)),
                    hour=int(mx.group(4)),
                    minute=int(mx.group(5)),
                    second=int(mx.group(6)),
                    microsecond=int(mx.group(7) or 0),
                    tzinfo=FixedOffset(mx.group(8)),
                )
    else:
        raise Exception("rx_time not matched in %r" % strtime)

    # .entry > {.title | .bylink}

    link = entry.find("a", {"class": "title"}) \
        or entry.find("a", {"class": "bylink"})
    purl = link["href"]
    if purl.startswith("/"):
        purl = "http://www.reddit.com" + purl

    # .entry > .usertext-body > .md

    txtbody = entry.find("div", {"class": "usertext-body"})
    if txtbody:
        text = txtbody.find("div", {"class": "md"})
    else:
        text = "(No content)"

    # output

    yield  {"url": url,
            "subject": title,
            "author": author,
            "date": ntime,
            "text": text}

TIMEFMT_MBOX = "%a %b %_d %H:%M:%S %Y"
TIMEFMT_MIME = "%a, %d %b %Y %H:%M:%S %z"

for url in sys.argv[1:]:
    for comment in scrape(url):
        mboxdate = comment["date"].strftime(TIMEFMT_MBOX)
        mimedate = comment["date"].strftime(TIMEFMT_MIME)

        msgid = "%s.%s@reddit" % (comment["author"],
                    comment["date"].isoformat())
        author = "%s@reddit" % comment["author"]

        print("From %s %s" % (author, mboxdate))
        print("URL: %s" % comment["url"])
        print("Message-ID: <%s>" % msgid)
        print("From: <%s>" % author)
        print("Date: %s (%s)" % (mimedate, comment["date"].isoformat()))
        print("Subject: %s" % comment["subject"])
        if comment["text"]:
            length = len(comment["text"]) + 1
            print("Content-Type: text/html; charset=utf-8")
            print("Content-Length: %d" % length)
            print("")
            print(comment["text"])
        print("")
        sys.stdout.flush()
